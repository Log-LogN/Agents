"""
Vulnerability Agent Internal Graph (Final)
=========================================
Tool-first LangGraph agent for vulnerability analysis.
Pattern: agent → tools (loop) → summarize
"""

import sys
import os
import logging
import time
import json
import ast
from typing import TypedDict, Annotated, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langchain_core.tools import BaseTool
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from shared.config import settings

logger = logging.getLogger("vulnerability-agent")


# =========================================================
# State
# =========================================================

class VulnerabilityAgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    final_output: str


# =========================================================
# Prompts
# =========================================================

VULNERABILITY_SYSTEM_PROMPT = SystemMessage(content="""
You are a cybersecurity vulnerability analysis assistant.

Always use tools to retrieve vulnerability data.

First, validate package versions using appropriate validator tools (tool_validate_npm, tool_validate_pypi, tool_validate_maven) to ensure the version exists before querying vulnerabilities.

Tool selection rules:

1. Maven dependency
   → First: tool_validate_maven (provide group if known, or "" to auto-discover), then use the returned group in tool_osv_maven_group or tool_cross_verify

2. Package with ecosystem (npm, PyPI, etc.)
   → First: tool_validate_npm or tool_validate_pypi, then tool_cross_verify (group="")

3. Software product + version (Apache, OpenSSL, Tomcat)
   → tool_product_cve (no validation needed for general products)

4. General vulnerability query
   → tool_cve_search

Never invent vulnerabilities.
Base your answer only on tool results.
""")

VULNERABILITY_SUMMARY_PROMPT = SystemMessage(content="""
Provide a detailed summary of the vulnerability analysis.

Include:
- Total vulnerabilities found
- Highest severity
- Key CVE or advisory IDs
- Risk impact (describe the potential consequences)
- Recommended action (upgrade / patch / monitor, with specific versions if possible)

Be comprehensive but organized.
""")


# =========================================================
# Helpers
# =========================================================

def _now_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def _extract_tool_output(content):
    try:
        if isinstance(content, dict):
            return content

        if isinstance(content, list):
            if content and "text" in content[0]:
                text = content[0]["text"]
                try:
                    return json.loads(text)
                except:
                    return text

        if isinstance(content, str):
            try:
                parsed = ast.literal_eval(content)
                if isinstance(parsed, list) and parsed and "text" in parsed[0]:
                    text = parsed[0]["text"]
                    try:
                        return json.loads(text)
                    except:
                        return text
            except:
                pass

        return content
    except Exception:
        return str(content)


# =========================================================
# Agent Execution
# =========================================================

async def run_vulnerability_agent(messages: List[BaseMessage], tools: List[BaseTool]) -> dict:
    logger.info(f"Vulnerability agent started: {messages[:100]}")

    if not tools:
        return {
            "output": "No vulnerability tools available.",
            "tool_calls": []
        }

    # LLM
    llm = ChatOpenAI(
        model=settings.OPENAI_MODEL,
        api_key=settings.OPENAI_API_KEY,
        temperature=0,
    )
    llm_with_tools = llm.bind_tools(tools)

    # =====================================================
    # Nodes
    # =====================================================

    async def reasoning_node(state: VulnerabilityAgentState):
        response = await llm_with_tools.ainvoke(
            [VULNERABILITY_SYSTEM_PROMPT] + state["messages"]
        )

        logger.info(f"Tool decision: {getattr(response, 'tool_calls', None)}")

        return {"messages": [response]}

    def should_continue(state: VulnerabilityAgentState):
        last = state["messages"][-1]
        if getattr(last, "tool_calls", None):
            return "tools"
        return "summarize"

    async def summarize_node(state: VulnerabilityAgentState):
        summary = await llm.ainvoke(
            [VULNERABILITY_SUMMARY_PROMPT] + state["messages"]
        )

        text = summary.content if isinstance(summary.content, str) else str(summary.content)

        return {
            "messages": [AIMessage(content=text)],
            "final_output": text,
        }

    # =====================================================
    # Graph
    # =====================================================

    graph = StateGraph(VulnerabilityAgentState)
    tool_node = ToolNode(tools)

    graph.add_node("reasoning", reasoning_node)
    graph.add_node("tools", tool_node)
    graph.add_node("summarize", summarize_node)

    graph.add_edge(START, "reasoning")

    graph.add_conditional_edges(
        "reasoning",
        should_continue,
        {
            "tools": "tools",
            "summarize": "summarize",
        }
    )

    # Loop for multiple tool calls
    graph.add_edge("tools", "reasoning")

    graph.add_edge("summarize", END)

    compiled_graph = graph.compile()

    # =====================================================
    # Execute
    # =====================================================

    try:
        initial_state = {"messages": messages}
        final_state = await compiled_graph.ainvoke(initial_state)

        output = final_state.get("final_output", "")

        # Extract tool calls
        tool_calls = []
        messages = final_state["messages"]

        for i, msg in enumerate(messages):
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                for tc in msg.tool_calls:
                    tool_output = ""
                    if i + 1 < len(messages):
                        next_msg = messages[i + 1]
                        if hasattr(next_msg, "content"):
                            tool_output = _extract_tool_output(next_msg.content)

                    tool_calls.append({
                        "tool_name": tc["name"],
                        "tool_input": tc["args"],
                        "tool_output": str(tool_output),
                    })

        return {
            "output": output,
            "tool_calls": tool_calls
        }

    except Exception as e:
        logger.exception("Vulnerability agent failed")
        return {
            "output": f"Execution failed: {str(e)}",
            "tool_calls": []
        }


# =========================================================
# Streaming
# =========================================================

async def run_vulnerability_agent_stream(message: str, tools: List[BaseTool]):
    yield {
        "event": "agent_started",
        "data": {"agent": "vulnerability", "message": message},
        "timestamp": _now_iso(),
    }

    try:
        result = await run_vulnerability_agent(message, tools)
        yield {
            "event": "agent_completed",
            "data": result,
            "timestamp": _now_iso(),
        }
    except Exception as e:
        yield {
            "event": "error",
            "data": {"error": str(e)},
            "timestamp": _now_iso(),
        }